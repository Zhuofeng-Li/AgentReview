{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62906f8a-6aef-4d48-8a3e-ba0b9c3d5b4b",
   "metadata": {},
   "outputs": [],
   "source": "import os\nCLIENT_TYPE = 'openai'\n# os.environ['OPENAI_API_KEY'] = 'your-api-key-here'  # Set your OpenAI key"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "664d2ade-94cb-44cc-9460-ba4092b8f311",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "\n",
    "args = Namespace(openai_client_type=CLIENT_TYPE, \n",
    "          api_version='2023-03-15-preview', \n",
    "          ac_scoring_method='ranking', \n",
    "          conference='ICLR2024', \n",
    "          num_reviewers_per_paper=3,  \n",
    "          ignore_missing_metareviews=False, \n",
    "          overwrite=False, \n",
    "          num_papers_per_area_chair=10, \n",
    "          model_name='gpt-4o', \n",
    "          output_dir='outputs', \n",
    "          max_num_words=16384, \n",
    "          visual_dir='outputs/visual', \n",
    "          device='cuda', \n",
    "          data_dir='./data', # Directory to all paper PDF\n",
    "          acceptance_rate=0.32, \n",
    "          skip_logging=True, # If set, we do not log the messages in the console.\n",
    "          task='paper_review')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "114d4525-3f47-4e2e-b91e-f7513ec4fa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "malicious_Rx1_setting = {\n",
    "    \"AC\": [\n",
    "        \"BASELINE\"\n",
    "    ],\n",
    "\n",
    "    \"reviewer\": [\n",
    "        \"malicious\",\n",
    "        \"BASELINE\",\n",
    "        \"BASELINE\"\n",
    "    ],\n",
    "\n",
    "    \"author\": [\n",
    "        \"BASELINE\"\n",
    "    ],\n",
    "    \"global_settings\":{\n",
    "        \"provides_numeric_rating\": ['reviewer', 'ac'],\n",
    "        \"persons_aware_of_authors_identities\": []\n",
    "    }\n",
    "}\n",
    "\n",
    "all_settings = {\"malicious_Rx1\": malicious_Rx1_setting}\n",
    "args.experiment_name = \"malicious_Rx1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e706786-4e0c-48f8-8d71-e1bbefeb1d8f",
   "metadata": {},
   "source": [
    "\n",
    "`malicious_Rx1` means 1 reviewer is a malicious reviewer, and the other reviewers are default (i.e. `BASELINE`) reviewers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ffecd4-4718-492e-b897-a5cceb6f3b6e",
   "metadata": {},
   "source": [
    "## Reviews\n",
    "\n",
    "Define the review pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e22ff91-d72a-412f-8c8d-52b9251ff566",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'agentreview'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      5\u001b[39m sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \u001b[33m\"\u001b[39m\u001b[33magentreview\u001b[39m\u001b[33m\"\u001b[39m)))\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01magentreview\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01menvironments\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PaperReview\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01magentreview\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpaper_review_arena\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PaperReviewArena\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01magentreview\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpaper_review_settings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_experiment_settings\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'agentreview'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"agentreview\")))\n",
    "\n",
    "from agentreview.environments import PaperReview\n",
    "from agentreview.paper_review_arena import PaperReviewArena\n",
    "from agentreview.paper_review_settings import get_experiment_settings\n",
    "from agentreview.utility.experiment_utils import initialize_players\n",
    "from agentreview.utility.utils import project_setup, get_paper_decision_mapping\n",
    "    \n",
    "from agentreview import const"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b7658b-742f-46d7-858a-684f3d8ce8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_one_paper(paper_id, setting):\n",
    "    args.task = \"paper_review\"\n",
    "    paper_decision = paper_id2decision[paper_id]\n",
    "\n",
    "    experiment_setting = get_experiment_settings(paper_id=paper_id,\n",
    "                                                 paper_decision=paper_decision,\n",
    "                                                 setting=setting)\n",
    "    print(f\"Paper ID: {paper_id} (Decision in {args.conference}: {paper_decision})\")\n",
    "\n",
    "    players = initialize_players(experiment_setting=experiment_setting, args=args)\n",
    "\n",
    "    player_names = [player.name for player in players]\n",
    "\n",
    "    env = PaperReview(player_names=player_names, paper_decision=paper_decision, paper_id=paper_id,\n",
    "                          args=args, experiment_setting=experiment_setting)\n",
    "\n",
    "    arena = PaperReviewArena(players=players, environment=env, args=args)\n",
    "    arena.launch_cli(interactive=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7cd3a8-bd7a-4c21-bfc7-fc7982a13a0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sampled_paper_ids = [39]\n",
    "sampled_paper_ids = [39, 247, 289, 400]\n",
    "\n",
    "paper_id2decision, paper_decision2ids = get_paper_decision_mapping(args.data_dir, args.conference)\n",
    "\n",
    "for paper_id in sampled_paper_ids:\n",
    "    review_one_paper(paper_id, malicious_Rx1_setting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de642af7-af85-46a8-9570-3dd599223d00",
   "metadata": {},
   "source": [
    "Note: Sometimes metareview fails to load due to content filtering. We thus use `experimental_paper_ids` to track the paper IDs that were actually used in the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1814583-3221-4a2a-a141-f20f5aae5906",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agentreview.environments import PaperDecision\n",
    "from agentreview.utility.utils import project_setup, get_paper_decision_mapping, \\\n",
    "    load_metareview, load_llm_ac_decisions_as_array\n",
    "\n",
    "args.task = \"paper_decision\"\n",
    "\n",
    "sampled_paper_ids = [39, 247, 289, 400]\n",
    "\n",
    "# Make sure the same set of papers always go through the same AC no matter which setting we choose\n",
    "NUM_PAPERS = len(sampled_paper_ids)\n",
    "order = np.random.choice(range(NUM_PAPERS), size=NUM_PAPERS, replace=False)\n",
    "\n",
    "\n",
    "# Paper IDs we actually used in experiments\n",
    "experimental_paper_ids = []\n",
    "\n",
    "# For papers that have not been decided yet, load their metareviews\n",
    "metareviews = []\n",
    "print(\"Shuffling paper IDs\")\n",
    "sampled_paper_ids = np.array(sampled_paper_ids)[order]\n",
    "\n",
    "for paper_id in sampled_paper_ids:\n",
    "    # Since we are feeding a batch of paper, the paper_id and paper_decision fields \n",
    "    # are not specific to one paper, thus left None\n",
    "    experiment_setting = get_experiment_settings(paper_id=None,\n",
    "                                                 paper_decision=None,\n",
    "                                                 setting=all_settings[args.experiment_name])\n",
    "\n",
    "    # Load meta-reviews\n",
    "    metareview = load_metareview(output_dir=args.output_dir, paper_id=paper_id,\n",
    "                                 experiment_name=args.experiment_name,\n",
    "                                 model_name=args.model_name, conference=args.conference)\n",
    "\n",
    "    if metareview is None:\n",
    "\n",
    "        print(f\"Metareview for {paper_id} does not exist. This may happen because the conversation is \"\n",
    "              f\"completely filtered out due to content policy. \"\n",
    "              f\"Loading the BASELINE metareview...\")\n",
    "\n",
    "        metareview = load_metareview(paper_id=paper_id, experiment_name=\"BASELINE\",\n",
    "                                     model_name=args.model_name, conference=args.conference)\n",
    "        print(metareview)\n",
    "\n",
    "    if metareview is not None:\n",
    "        metareviews += [metareview]\n",
    "        experimental_paper_ids += [paper_id]\n",
    "\n",
    "args.num_papers_per_area_chair = 2\n",
    "num_batches = len(experimental_paper_ids) // args.num_papers_per_area_chair\n",
    "\n",
    "for batch_index in range(num_batches):\n",
    "\n",
    "    players = initialize_players(experiment_setting=experiment_setting, args=args)\n",
    "    player_names = [player.name for player in players]\n",
    "\n",
    "    if batch_index >= num_batches - 1:  # Last batch. Include all remaining papers\n",
    "        batch_paper_ids = experimental_paper_ids[batch_index * args.num_papers_per_area_chair:]\n",
    "\n",
    "    else:\n",
    "        batch_paper_ids = experimental_paper_ids[batch_index * args.num_papers_per_area_chair: (batch_index + 1) *\n",
    "                                                                                               args.num_papers_per_area_chair]\n",
    "\n",
    "    env = PaperDecision(player_names=player_names, paper_ids=batch_paper_ids,\n",
    "                        metareviews=metareviews,\n",
    "                        experiment_setting=experiment_setting, ac_scoring_method=args.ac_scoring_method)\n",
    "\n",
    "    arena = PaperReviewArena(players=players, environment=env, args=args, global_prompt=const.GLOBAL_PROMPT)\n",
    "    arena.launch_cli(interactive=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3eb359-2814-49ac-bf2b-b13219ddb3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "decisions, paper_ids = load_llm_ac_decisions_as_array(output_dir=args.output_dir, conference=args.conference, \n",
    "                                                      model_name=args.model_name,\n",
    "                                                      ac_scoring_method=args.ac_scoring_method,\n",
    "                                                      experiment_name=args.experiment_name,\n",
    "                                                      acceptance_rate=args.acceptance_rate,\n",
    "                                                      num_papers_per_area_chair=args.num_papers_per_area_chair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fe60ab-8324-4632-b84b-ac2a7b560a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for paper_id, decision in zip(paper_ids, decisions):\n",
    "    print(f\"{paper_id}\\t{'Accept' if decision else 'Reject'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}